import os
import shutil
import stat
import tempfile
import time
import json
import hashlib
import sqlite3

import pandas as pd

from typing import List, Dict, Optional, Callable, Any  # + Callable
from langchain_classic.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
# from langchain.vectorstores import Chroma
from langchain_classic.schema import Document
from langchain_community.document_loaders import TextLoader, DirectoryLoader
#from langchain_community.vectorstores import Chroma
from langchain_chroma import Chroma
from filelock import FileLock
from chromadb.config import Settings  # added
#from settings import settings  # –¥–æ–±–∞–≤–ª–µ–Ω–æ

class VectorStore:
    def __init__(
        self,
        docs_path: str = "./data/docs/",
        vector_store_path: str = "./data/vector_store",
        openai_api_key: str =  os.getenv("OPENAI_API_KEY"),
        chunk_size: int = 5000,
        chunk_overlap: int = 500
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        
        Args:
            docs_path: –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            vector_store_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
            openai_api_key: API –∫–ª—é—á OpenAI
            chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            chunk_overlap: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π fallback)
        self.docs_path = docs_path or os.getenv("DOCS_PATH")# or settings.DOCS_PATH
        self.vector_store_path = vector_store_path or os.getenv("VECTOR_STORE_PATH", "/app/data/vector_store")
        
        # –ü—É—Ç—å –¥–ª—è —Ñ–∞–π–ª–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        self.lock_file_path = f"{self.vector_store_path}.lock"
        
        print(f"–¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
        self.docs_path = os.path.abspath(self.docs_path)
        self.vector_store_path = os.path.abspath(self.vector_store_path)
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–∫–æ–ª–±–µ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.progress_callback: Optional[Callable[[str], None]] = None

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ Excel
        self.excel_max_rows = int(os.getenv("EXCEL_MAX_ROWS", "2000"))
        self.excel_output_format = (
            os.getenv("EXCEL_NORMALIZED_FORMAT", "markdown") or "markdown"
        ).strip().lower()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–ø–∏–π –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.save_normalized_docs = str(os.getenv("SAVE_NORMALIZED_DOCS", "false")).lower() in ("1", "true", "yes")
        self.save_only_changed = str(os.getenv("SAVE_ONLY_CHANGED", "true")).lower() in ("1", "true", "yes")
        normalized_root = os.getenv("NORMALIZED_OUTPUT_DIR", "/app/data/normalized")
        self.normalized_base_dir = os.path.abspath(normalized_root)
        self.normalized_output_dir = self._resolve_normalized_output_dir()
        self.include_normalized_docs = str(
            os.getenv("INCLUDE_NORMALIZED_DOCS", "true")
        ).lower() in ("1", "true", "yes")
        if self.save_normalized_docs or self.include_normalized_docs:
            try:
                os.makedirs(self.normalized_output_dir, exist_ok=True)
            except Exception as e:
                msg = (
                    "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: "
                    f"{e}"
                )
                if self.save_normalized_docs:
                    raise PermissionError(msg) from e
                print(msg)

        # –Ø–≤–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –∞–Ω–æ–Ω–∏–º–Ω—É—é —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é Chroma (–≤–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ —Å–±–æ–µ–≤ –≤ CI)
        os.environ.setdefault("ANONYMIZED_TELEMETRY", "False")
        self.client_settings = Settings(
            anonymized_telemetry=False,
            allow_reset=True,
            is_persistent=True,
        )

        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=self.openai_api_key
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )

        self.vectorstore = None

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∞–≤–∞–º–∏
        self._ensure_directories()

    def set_progress_callback(self, cb: Optional[Callable[[str], None]]):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–ª–±–µ–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (—Å—Ç—Ä–æ–∫–∞ ‚Üí —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ)."""
        self.progress_callback = cb

    def _notify(self, msg: str):
        """–õ–æ–∫–∞–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: –ø–µ—á–∞—Ç—å + –≤–Ω–µ—à–Ω–∏–π –∫–æ–ª–±–µ–∫ (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)."""
        try:
            print(msg)
        finally:
            if self.progress_callback:
                try:
                    self.progress_callback(msg)
                except Exception:
                    # –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∞–ª–∏–º –ø—Ä–æ—Ü–µ—Å—Å –∏–∑-–∑–∞ –∫–æ–ª–±–µ–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    pass

    def _resolve_normalized_output_dir(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞."""
        docs_root_env = os.getenv("DOCS_PATH")# or settings.DOCS_PATH
        docs_abs = self.docs_path
        normalized_rel: Optional[str] = None

        if docs_root_env:
            docs_root_abs = os.path.abspath(docs_root_env)
            try:
                rel_candidate = os.path.relpath(docs_abs, start=docs_root_abs)
                if not rel_candidate.startswith(".."):
                    normalized_rel = rel_candidate if rel_candidate != "." else ""
            except Exception:
                normalized_rel = None

        if not normalized_rel:
            basename = os.path.basename(docs_abs.rstrip(os.sep))
            normalized_rel = basename or "product"

        return os.path.join(self.normalized_base_dir, normalized_rel)

    def _rel_path_from_docs(self, src: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–∞—Ç–∞–ª–æ–≥–∞ docs –¥–æ —Ñ–∞–π–ª–∞ src."""
        try:
            return os.path.relpath(src, start=self.docs_path)
        except Exception:
            return os.path.basename(src) if src else "unknown"

    def _build_norm_path(self, doc: Document, changed: bool, idx: int, force: bool = False) -> Optional[str]:
        """–°—Ç—Ä–æ–∏—Ç –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —É—á–∏—Ç—ã–≤–∞—è Excel-–ª–∏—Å—Ç—ã –∏ –∑–µ—Ä–∫–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None, –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ (save_only_changed –∏ –Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π),
        –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (force=True).
        """
        if self.save_only_changed and not changed and not force:
            return None
        src = doc.metadata.get("source")
        rel = self._rel_path_from_docs(src) if src else f"unknown_{idx}.txt"
        rel_dir = os.path.dirname(rel)
        base_name = os.path.basename(rel)
        # –î–ª—è Excel –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å —Å –∏–º–µ–Ω–µ–º –ª–∏—Å—Ç–∞
        if doc.metadata.get("excel"):
            sheet = str(doc.metadata.get("sheet_name", "Sheet")).replace("/", "_").replace("\\", "_")
            out_name = f"{base_name}__{sheet}.norm.txt"
        else:
            out_name = f"{base_name}.norm.txt"
        out_dir = os.path.join(self.normalized_output_dir, rel_dir)
        try:
            os.makedirs(out_dir, exist_ok=True)
        except Exception:
            pass
        return os.path.join(out_dir, out_name)

    def _save_normalized_copy(
        self,
        doc: Document,
        new_text: str,
        changed: bool,
        idx: int,
        force: bool = False,
    ) -> Optional[str]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –¥–∏—Å–∫ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∏–ª–∏ None."""
        if not self.save_normalized_docs:
            return None
        path = self._build_norm_path(doc, changed, idx, force=force)
        if not path:
            return None
        try:
            with open(path, "w", encoding="utf-8") as f:
                f.write(new_text)
            try:
                rel = os.path.relpath(path, start=self.normalized_output_dir)
            except Exception:
                rel = path
            self._notify(f"[norm] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {rel}")
            return path
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª '{path}': {e}")
            return None

    def _save_raw_documents(self, documents: List[Document]) -> None:
        """–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–µ–∏–∑–º—ë–Ω–Ω—ã–µ –∫–æ–ø–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –¥–∏—Å–∫."""
        if not self.save_normalized_docs:
            return

        self._notify("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–ø–∏–π –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        processed = 0
        saved_files = 0
        start_all = time.time()
        for doc in documents:
            try:
                content = doc.page_content if doc.page_content is not None else ""
                if content == "":
                    continue
                processed += 1
                src = doc.metadata.get("source", "unknown")
                fname = os.path.basename(src) if src and src != "unknown" else f"doc_{processed}"
                self._notify(f"[norm] ‚ñ∂Ô∏è {fname}: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")

                saved_path = self._save_normalized_copy(
                    doc,
                    new_text=content,
                    changed=False,
                    idx=processed,
                    force=True,
                )
                if saved_path:
                    saved_files += 1
            except Exception as e:
                print(
                    f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—â–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                    f" ({doc.metadata.get('source', 'unknown')}): {e}"
                )
        total_time = time.time() - start_all
        self._notify(
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {processed} | –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {saved_files}"
            f" | –≤—Ä–µ–º—è –≤—Å–µ–≥–æ={total_time:.2f}s"
        )

    def _ensure_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        try:
            os.makedirs(self.docs_path, exist_ok=True)
        except OSError as exc:
            raise PermissionError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ {self.docs_path}: {exc}"
            ) from exc

        try:
            os.makedirs(self.vector_store_path, exist_ok=True)
        except OSError as exc:
            raise PermissionError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ {self.vector_store_path}: {exc}"
            ) from exc

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∑–∞–ø–∏—Å–∏ (–æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º read-only –º–∞—É–Ω—Ç—ã –∑–∞—Ä–∞–Ω–µ–µ)
        self._assert_writable_dir(
            self.vector_store_path,
            reason="–∑–∞–ø–∏—Å—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞",
        )
        parent_dir = os.path.dirname(self.vector_store_path) or self.vector_store_path
        self._assert_writable_dir(
            parent_dir,
            reason="—Å–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏",
        )

        if self.save_normalized_docs:
            self._assert_writable_dir(
                self.normalized_output_dir,
                reason="—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤",
            )

    def _is_learning_source(self, source: Optional[str]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å learn)."""
        if not source:
            return False
        base_name = os.path.basename(source).lower()
        if base_name.startswith("learn"):
            return True
        parent = os.path.basename(os.path.dirname(source)).lower()
        return parent.startswith("learn")

    def _mark_learning_documents(self, documents: List[Document]) -> None:
        """–ü–æ–º–µ—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        for doc in documents:
            source = doc.metadata.get("source")
            if self._is_learning_source(source):
                doc.metadata["learning_doc"] = True
                doc.metadata.setdefault("priority", "learn")

    def _doc_priority_rank(self, doc: Document) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–º–µ–Ω—å—à–µ = –≤–∞–∂–Ω–µ–µ)."""
        if doc.metadata.get("learning_doc"):
            return 0
        if doc.metadata.get("normalized"):
            return 1
        return 2

    def _sort_documents_by_priority(self, documents: List[Document]) -> List[Document]:
        """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —É—á—ë—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ (—Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –≤—ã—à–µ –≤—Å–µ–≥–æ)."""
        return sorted(
            documents,
            key=lambda doc: (self._doc_priority_rank(doc), doc.metadata.get("source", ""))
        )

    def _assert_writable_dir(self, directory: str, reason: str = "") -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, —Å–æ–∑–¥–∞–≤–∞—è –∏ —É–¥–∞–ª—è—è –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª."""
        try:
            os.makedirs(directory, exist_ok=True)
        except OSError as exc:
            raise PermissionError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {directory}: {exc}"
            ) from exc

        hint = f" ({reason})" if reason else ""
        try:
            fd, tmp_path = tempfile.mkstemp(prefix=".writetest_", dir=directory)
            os.close(fd)
            os.unlink(tmp_path)
        except OSError as exc:
            raise PermissionError(
                f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {directory}{hint}: {exc}"
            ) from exc

    # === Excel helpers ===
    def _sanitize_markdown_cell(self, value: Optional[str]) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —è—á–µ–π–∫–∏ –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–º—É –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –≤ Markdown-—Ç–∞–±–ª–∏—Ü–µ."""
        if value is None:
            return ""
        sanitized = str(value)
        sanitized = sanitized.replace("\r\n", "\n").replace("\r", "\n").strip()
        sanitized = sanitized.replace("|", "\\|")
        return sanitized.replace("\n", "<br>")

    def _df_to_markdown(self, df: "pd.DataFrame") -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç Markdown-—Ç–∞–±–ª–∏—Ü—É –∏–∑ DataFrame –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        safe_df = df.fillna("")
        headers = [
            self._sanitize_markdown_cell(str(col) if str(col) else f"Column {idx + 1}")
            for idx, col in enumerate(safe_df.columns)
        ]
        lines = [
            "| " + " | ".join(headers) + " |",
            "| " + " | ".join("---" for _ in headers) + " |",
        ]
        for _, row in safe_df.iterrows():
            cells = [self._sanitize_markdown_cell(row[col]) for col in safe_df.columns]
            lines.append("| " + " | ".join(cells) + " |")
        return "\n".join(lines)

    def _df_to_bullet(self, df: "pd.DataFrame") -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Ç–∞–±–ª–∏—Ü—ã –≤ –≤–∏–¥–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞."""
        safe_df = df.fillna("")
        records = safe_df.to_dict(orient="records")
        lines = []
        for idx, record in enumerate(records, start=1):
            lines.append(f"–°—Ç—Ä–æ–∫–∞ {idx}:")
            for column in safe_df.columns:
                value = str(record.get(column, "")).strip()
                if not value:
                    continue
                value = value.replace("\r\n", " ").replace("\n", " ")
                lines.append(f"  - {column}: {value}")
            lines.append("")
        return "\n".join(lines).strip()

    def _df_to_tsv(self, df: "pd.DataFrame") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç TSV-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)."""
        return df.to_csv(sep="\t", index=False)

    def _format_excel_table(self, df: "pd.DataFrame") -> (str, str):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç + –ø—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."""
        requested = (self.excel_output_format or "markdown").lower()
        aliases = {
            "md": "markdown",
            "table": "markdown",
            "list": "bullet",
        }
        fmt = aliases.get(requested, requested)

        try:
            if fmt == "markdown":
                return self._df_to_markdown(df), "markdown"
            if fmt == "bullet":
                return self._df_to_bullet(df), "bullet"
            if fmt == "csv":
                return df.to_csv(index=False), "csv"
            if fmt == "tsv":
                return self._df_to_tsv(df), "tsv"
            self._notify(f"[norm] ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Excel '{requested}', –∏—Å–ø–æ–ª—å–∑—É–µ–º TSV")
            return self._df_to_tsv(df), "tsv"
        except Exception as e:
            self._notify(
                f"[norm] ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Excel ({fmt}): {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º TSV"
            )
            return self._df_to_tsv(df), "tsv"

    def _excel_to_text(self, file_path: str) -> List[Document]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç .xlsx –≤ —Å–ø–∏—Å–æ–∫ Document (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –ª–∏—Å—Ç).
        –¢–µ–∫—Å—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π CSV/TSV –±–µ–∑ –∏–Ω–¥–µ–∫—Å–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫.
        """
        docs: List[Document] = []
        try:
            import pandas as pd  # –∏–º–ø–æ—Ä—Ç –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –±–µ–∑ pandas
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: pandas –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Excel: {e}")
            return docs

        try:
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Ç–∏–ø–∞–º)
            xls = pd.read_excel(file_path, sheet_name=None, dtype=str, engine="openpyxl")
            total_sheets = len(xls)
            self._notify(f"–ó–∞–≥—Ä—É–∑–∫–∞ Excel: {os.path.basename(file_path)} | –ª–∏—Å—Ç–æ–≤: {total_sheets}")
            for sheet_name, df in xls.items():
                if df is None:
                    continue
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º NaN
                df = df.fillna("")
                # –û–≥—Ä–∞–Ω–∏—á–∏–º —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
                max_rows = self.excel_max_rows
                rows_before = len(df)
                if rows_before > max_rows:
                    df = df.head(max_rows)
                table_text, used_format = self._format_excel_table(df)
                format_label = {
                    "markdown": "Markdown",
                    "bullet": "bullet list",
                    "csv": "CSV",
                    "tsv": "TSV",
                }.get(used_format, used_format)
                header = (
                    f"–ò—Å—Ç–æ—á–Ω–∏–∫: {os.path.basename(file_path)} | –õ–∏—Å—Ç: {sheet_name} | –°—Ç—Ä–æ–∫: {len(df)}"
                    + (f" (–æ–±—Ä–µ–∑–∞–Ω–æ –∏–∑ {rows_before})" if rows_before > len(df) else "")
                    + f" | –§–æ—Ä–º–∞—Ç —Ç–∞–±–ª–∏—Ü—ã: {format_label}"
                )
                full_text = header + "\n\n" + table_text
                docs.append(
                    Document(
                        page_content=full_text,
                        metadata={
                            "source": file_path,
                            "sheet_name": str(sheet_name),
                            "rows": len(df),
                            "excel": True,
                        },
                    )
                )
            return docs
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel '{file_path}': {e}")
            return docs

    def _load_excel_documents(self) -> List[Document]:
        """–ü–æ–∏—Å–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ .xlsx –∏–∑ self.docs_path —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ."""
        excel_docs: List[Document] = []
        try:
            for root, _, files in os.walk(self.docs_path):
                for name in files:
                    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ .xlsx (–±–µ–∑ .xls, —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å xlrd)
                    if name.lower().endswith(".xlsx"):
                        path = os.path.join(root, name)
                        docs = self._excel_to_text(path)
                        excel_docs.extend(docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ Excel: {e}")
        self._notify(f"Excel: —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(excel_docs)} –¥–æ–∫—É–º–µ–Ω—Ç(–æ–≤) –∏–∑ .xlsx —Ñ–∞–π–ª–æ–≤")
        return excel_docs

    def _clear_vector_store(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if os.path.exists(self.vector_store_path):
            shutil.rmtree(self.vector_store_path)

    def _load_normalized_documents(self) -> List[Document]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
        if not self.include_normalized_docs:
            return []
        if not os.path.isdir(self.normalized_output_dir):
            return []

        loader = DirectoryLoader(
            self.normalized_output_dir,
            glob="**/*.norm.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'},
        )

        try:
            docs = loader.load()
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
            return []

        for doc in docs:
            doc.metadata["normalized"] = True
        return docs

    def _shutdown_chroma_client(self, store: Any) -> None:
        """Attempt to stop embedded Chroma client to release file handles."""
        if store is None:
            return
        try:
            client = getattr(store, "_client", None)
            system = getattr(client, "_system", None)
            stop = getattr(system, "stop", None)
            if callable(stop):
                stop()
        except Exception as exc:
            print(f"[chroma] Failed to stop client cleanly: {exc}")

    def _force_close_connections(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π ChromaDB"""
        try:
            import gc
            import threading
            import subprocess
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å (–±–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö reset/stop)
            if hasattr(self, 'vectorstore') and self.vectorstore is not None:
                try:
                    self._shutdown_chroma_client(self.vectorstore)
                except Exception as stop_error:
                    print(f"[chroma] Could not stop active store cleanly: {stop_error}")
                try:
                    # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –æ–±—ä–µ–∫—Ç ‚Äî —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
                    del self.vectorstore
                    self.vectorstore = None
                except Exception as e:
                    print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ vectorstore: {e}")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ lsof
            try:
                # –ò—â–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                result = subprocess.run(
                    ['lsof', '+D', self.vector_store_path],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0 and result.stdout.strip():
                    print(f"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Ñ–∞–π–ª—ã: {result.stdout}")
                    # –ü—Ä–æ—Å—Ç–æ –¥–∞–µ–º –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ
                    time.sleep(3)
                else:
                    print("–§–∞–π–ª—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏")
            except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
                # lsof –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ —Ç–∞–π–º–∞—É—Ç
                pass
            
            # –î–∞–µ–º –≤—Ä–µ–º—è —Å–∏—Å—Ç–µ–º–µ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
            time.sleep(3)
            
            print("–°–æ–µ–¥–∏–Ω–µ–Ω–∏—è ChromaDB –∑–∞–∫—Ä—ã—Ç—ã")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–º –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {e}")

    def _safe_remove_directory(self, directory_path: str, max_attempts: int = 5):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        import subprocess
        
        for attempt in range(max_attempts):
            try:
                if os.path.exists(directory_path):
                    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
                    for root, dirs, files in os.walk(directory_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            try:
                                os.chmod(file_path, stat.S_IWRITE | stat.S_IREAD)
                            except:
                                pass
                    
                    # –£–¥–∞–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                    shutil.rmtree(directory_path)
                    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory_path} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞")
                    return
            except OSError as e:
                if e.errno == 16 and attempt < max_attempts - 1:  # Device or resource busy
                    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–Ω—è—Ç–∞, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}. –ñ–¥–µ–º...")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
                    if attempt >= 2:  # –ü–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–π –ø–æ–ø—ã—Ç–∫–∏ —Å—Ç–∞–Ω–æ–≤–∏–º—Å—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏
                        try:
                            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Ñ–∞–π–ª—ã
                            result = subprocess.run(
                                ['lsof', '+D', directory_path],
                                capture_output=True, text=True, timeout=5
                            )
                            if result.returncode == 0 and result.stdout.strip():
                                print("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Ñ–∞–π–ª—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
                        except:
                            pass
                    
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    wait_time = min(2 ** attempt, 10)
                    time.sleep(wait_time)
                    continue
                elif attempt == max_attempts - 1:
                    print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ, –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É
                    try:
                        backup_path = f"{directory_path}_backup_{int(time.time())}"
                        os.rename(directory_path, backup_path)
                        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ {backup_path} –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è")
                        return
                    except:
                        raise e
                else:
                    raise

    def _load_documents(self) -> List[Document]:
        print("\n===== –§–ê–ô–õ–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê =====")
        print(f"–ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {self.docs_path}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not os.path.exists(self.docs_path):
            print(f"–û–®–ò–ë–ö–ê: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.docs_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            return []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        files = os.listdir(self.docs_path)
        print(f"–§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {files}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∏–ø–æ–≤
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        documents: List[Document] = []

        # TXT
        txt_loader = DirectoryLoader(
            self.docs_path,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        txt_docs = []
        try:
            txt_docs = txt_loader.load()
            documents.extend(txt_docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ .txt: {e}")

        # DOCX (—á–µ—Ä–µ–∑ python-docx)
        docx_docs = []
        try:
            from docx import Document
            from langchain_classic.schema import Document as LangchainDocument
            
            for root, dirs, files in os.walk(self.docs_path):
                for file in files:
                    if file.endswith('.docx'):
                        file_path = os.path.join(root, file)
                        try:
                            doc = Document(file_path)
                            text = '\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])
                            if text.strip():
                                docx_docs.append(LangchainDocument(
                                    page_content=text,
                                    metadata={"source": file_path}
                                ))
                        except Exception as file_e:
                            print(f"Error loading file {file_path}: {file_e}")
            documents.extend(docx_docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å .docx (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ python-docx): {e}")

        # PDF (—á–µ—Ä–µ–∑ unstructured)
        pdf_docs = []
        try:
            from langchain_community.document_loaders import UnstructuredPDFLoader
            pdf_loader = DirectoryLoader(
                self.docs_path,
                glob="**/*.pdf",
                loader_cls=UnstructuredPDFLoader,
            )
            pdf_docs = pdf_loader.load()
            documents.extend(pdf_docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å .pdf (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ pypdf/unstructured): {e}")

        # XLSX (—á–µ—Ä–µ–∑ pandas ‚Üí —Ç–µ–∫—Å—Ç)
        excel_docs = []
        try:
            excel_docs = self._load_excel_documents()
            documents.extend(excel_docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ Excel: {e}")

        normalized_docs: List[Document] = []
        try:
            normalized_docs = self._load_normalized_documents()
            documents.extend(normalized_docs)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")

        # –ü–æ–º–µ—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self._mark_learning_documents(documents)

        unique_docs: List[Document] = []
        index_by_hash: Dict[str, int] = {}
        for doc in documents:
            content = doc.page_content or ""
            content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()
            if content_hash in index_by_hash:
                existing_idx = index_by_hash[content_hash]
                existing_doc = unique_docs[existing_idx]
                if self._doc_priority_rank(doc) < self._doc_priority_rank(existing_doc):
                    unique_docs[existing_idx] = doc
                continue
            index_by_hash[content_hash] = len(unique_docs)
            unique_docs.append(doc)

        deduplicated = len(documents) - len(unique_docs)
        if deduplicated:
            self._notify(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {deduplicated}")

        documents = self._sort_documents_by_priority(unique_docs)

        self._notify(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (txt: {len(txt_docs)}, docx: {len(docx_docs)}, pdf: {len(pdf_docs)}, "
            f"excel: {len(excel_docs)}, normalized: {len(normalized_docs)})"
        )
        return documents

    def _split_documents(self, documents: List[Document]) -> List[Document]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = self.text_splitter.split_documents(documents)
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks

    def create(self, force_recreate: bool = True) -> 'VectorStore':
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        self._notify("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞
        self._ensure_directories()

        if not force_recreate:
            try:
                if not self.is_empty():
                    self._notify(
                        "–•—Ä–∞–Ω–∏–ª–∏—â–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É (force_recreate=False)"
                    )
                    return self.load()
            except Exception:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–µ—Ä–µ—Å–±–æ—Ä–∫–æ–π
                pass

        # Safe rebuild —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å –¥–æ —É—Å–ø–µ—à–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏
        return self.safe_rebuild()

    def load(self) -> 'VectorStore':
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if not os.path.exists(self.vector_store_path):
            raise ValueError(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ {self.vector_store_path}")

        if not os.access(self.vector_store_path, os.W_OK):
            raise PermissionError(f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {self.vector_store_path}")

        self.vectorstore = Chroma(
            persist_directory=self.vector_store_path,
            embedding_function=self.embeddings,
            client_settings=self.client_settings,
        )

        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
        return self

    def search(self, query: str, n_results: int = 6, product: str = "default") -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            n_results: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List[Dict]: —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        print(self.vectorstore)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        if not self.vectorstore:
            print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å...")
            try:
                self.load()
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
                return []

        try:
            search_kwargs = None
            if product != "default":
                search_kwargs={"product": {"$eq": product}}
            search_result = self.vectorstore.similarity_search_with_relevance_scores(
                query,
                k=n_results,
                filter=search_kwargs
            )
            docs = [Document(page_content=d.page_content, metadata={**(d.metadata or {}), "relevance_score": float(s)}) for d, s in search_result if (s or 0) > 0]
            if not docs:
                return []

            # –ù–∞ —Å–ª—É—á–∞–π —Å—Ç–∞—Ä—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ–º–µ—á–∞–µ–º learning –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫—É
            self._mark_learning_documents(docs)

            prioritized = self._sort_documents_by_priority(docs)
            return prioritized[:n_results]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
            return []

    def safe_rebuild(self) -> 'VectorStore':
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        self._notify("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        # –ü–æ–≤—Ç–æ—Ä–Ω–æ —É–¥–æ—Å—Ç–æ–≤–µ—Ä—è–µ–º—Å—è, —á—Ç–æ –º–æ–∂–µ–º –ø–∏—Å–∞—Ç—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–º–æ–≥–ª–∏ —Å–º–µ–Ω–∏—Ç—å—Å—è –ø—Ä–∞–≤–∞ –∏–ª–∏ –º–∞—É–Ω—Ç)
        self._ensure_directories()

        # –°–æ–∑–¥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        with FileLock(self.lock_file_path, timeout=30):
            try:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                new_store_path = f"{self.vector_store_path}_new_{int(time.time())}"
                self._notify(f"–°–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤: {new_store_path}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
                documents = self._load_documents()
                if not documents:
                    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ø–∏–∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                self._save_raw_documents(documents)

                chunks = self._split_documents(documents)
                if not chunks:
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –Ω–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                self._notify("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
                new_vectorstore = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=new_store_path,
                    client_settings=self.client_settings,
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                #new_vectorstore.persist()
                self._notify("–ù–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
                
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤–æ–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
                self._shutdown_chroma_client(new_vectorstore)
                del new_vectorstore
                
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                if self.vectorstore:
                    self._notify("–ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
                    self._force_close_connections()
                
                # –°–æ–∑–¥–∞–µ–º backup —Å—Ç–∞—Ä–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –µ—Å–ª–∏ –æ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                backup_path = None
                if os.path.exists(self.vector_store_path):
                    backup_path = f"{self.vector_store_path}_backup_{int(time.time())}"
                    self._notify(f"–°–æ–∑–¥–∞–µ–º backup —Å—Ç–∞—Ä–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {backup_path}")
                    try:
                        shutil.move(self.vector_store_path, backup_path)
                    except Exception as e:
                        self._notify(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å backup: {e}")
                        # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        backup_path = None
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞ –º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ
                self._notify(f"–ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
                try:
                    shutil.move(new_store_path, self.vector_store_path)
                    self._notify("–ù–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —É—Å–ø–µ—à–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ")
                except Exception as e:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º backup
                    self._notify(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
                    if backup_path and os.path.exists(backup_path):
                        self._notify("–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º backup...")
                        shutil.move(backup_path, self.vector_store_path)
                        backup_path = None
                    raise
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                self.load()
                self._notify("–ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                
                # –£–¥–∞–ª—è–µ–º backup –µ—Å–ª–∏ –≤—Å–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ
                if backup_path and os.path.exists(backup_path):
                    try:
                        self._notify(f"–£–¥–∞–ª—è–µ–º backup: {backup_path}")
                        shutil.rmtree(backup_path)
                    except Exception as e:
                        self._notify(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å backup {backup_path}: {e}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞
                self._save_manifest(self._gather_source_files())
                
                return self
                
            except Exception as e:
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                if 'new_store_path' in locals() and os.path.exists(new_store_path):
                    try:
                        shutil.rmtree(new_store_path)
                        print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {new_store_path} –æ—á–∏—â–µ–Ω–∞")
                    except Exception as cleanup_error:
                        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {cleanup_error}")
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –ø–æ–ø—ã—Ç–∫–æ–π –∑–∞–ø–∏—Å–∏ –≤ read-only SQLite, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –µ—ë –≤ PermissionError
                if isinstance(e, sqlite3.OperationalError) and "readonly" in str(e).lower():
                    readonly_msg = (
                        "–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∑–∞–ø–∏—Å–∏ (read-only). "
                        "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –Ω–∞ –∫–∞—Ç–∞–ª–æ–≥ –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∞—É–Ω—Ç–∞ volume."
                    )
                    self._notify(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {readonly_msg}")
                    raise PermissionError(readonly_msg) from e

                self._notify(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
                raise
                
    def is_empty(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—É—Å—Ç–æ –ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (—Ñ–∞–π–ª–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –∑–∞–ø—É—Å–∫–∞ Chroma)"""
        if not os.path.exists(self.vector_store_path):
            return True
            
        try:
            files = os.listdir(self.vector_store_path)
            if len(files) == 0:
                return True

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∞–π–ª–æ–≤ Chroma (sqlite-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ)
            sqlite_path = os.path.join(self.vector_store_path, 'chroma.sqlite3')
            has_sqlite = os.path.isfile(sqlite_path) and os.path.getsize(sqlite_path) > 0

            # –ß–∞—Å—Ç–æ —Ä—è–¥–æ–º —Å –ë–î –µ—Å—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (UUID)
            has_any_dir = any(
                os.path.isdir(os.path.join(self.vector_store_path, f)) for f in files
            )

            if has_sqlite:
                print(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–π–ª—ã: {files}")
                return False

            # –ï—Å–ª–∏ –Ω–µ—Ç sqlite, –Ω–æ –µ—Å—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ‚Äî –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—á–∏—Ç–∞–µ–º –ø—É—Å—Ç—ã–º/–±–∏—Ç—ã–º
            if not has_sqlite and has_any_dir:
                print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω–æ–π –ë–î chroma.sqlite3")
                return True

            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
            return True

    # ================== –ú–ê–ù–ò–§–ï–°–¢ / –ü–†–û–í–ï–†–ö–ê –ê–ö–¢–£–ê–õ–¨–ù–û–°–¢–ò ==================
    @property
    def _manifest_path(self) -> str:
        return os.path.join(self.vector_store_path, "_manifest.json")

    def _gather_source_files(self) -> List[Dict[str, str]]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç self.docs_path –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å mtime –∏ —Ä–∞–∑–º–µ—Ä–æ–º.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: .txt, .docx, .xlsx, .pdf
        """
        exts = {".txt", ".docx", ".xlsx", ".pdf"}
        results: List[Dict[str, str]] = []
        if not os.path.isdir(self.docs_path):
            return results
        for root, _, files in os.walk(self.docs_path):
            for f in files:
                _, ext = os.path.splitext(f.lower())
                if ext in exts:
                    full = os.path.join(root, f)
                    try:
                        st = os.stat(full)
                        rel = os.path.relpath(full, self.docs_path)
                        results.append({
                            "rel": rel,
                            "mtime": str(int(st.st_mtime)),
                            "size": str(st.st_size),
                        })
                    except Exception:
                        pass
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö–µ—à–∞
        results.sort(key=lambda x: x["rel"])
        return results

    def _compute_signature(self, items: List[Dict[str, str]]) -> str:
        h = hashlib.sha256()
        for it in items:
            h.update(it["rel"].encode("utf-8"))
            h.update(b"|")
            h.update(it["mtime"].encode("utf-8"))
            h.update(b"|")
            h.update(it["size"].encode("utf-8"))
            h.update(b"\n")
        return h.hexdigest()

    def _load_manifest(self) -> Optional[Dict]:
        try:
            if os.path.isfile(self._manifest_path):
                with open(self._manifest_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception:
            return None
        return None

    def _save_manifest(self, items: List[Dict[str, str]]):
        data = {
            "files": items,
            "signature": self._compute_signature(items),
            "timestamp": int(time.time()),
        }
        try:
            with open(self._manifest_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç: {e}")

    def needs_rebuild(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏.
        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–ª–∏ chroma.sqlite3
        - –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º–∞–Ω–∏—Ñ–µ—Å—Ç
        - –ü–æ–¥–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤ –≤ docs_path –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (–∫–æ–ª-–≤–æ, –∏–º–µ–Ω–∞, mtime, —Ä–∞–∑–º–µ—Ä)
        - –ï—Å—Ç—å —Ñ–∞–π–ª—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∞ –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ –±—ã–ª–æ 0 (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π stub)
        - –§–∞–π–ª—ã –≤ docs –Ω–æ–≤–µ–µ chroma.sqlite3
        """
        sqlite_path = os.path.join(self.vector_store_path, "chroma.sqlite3")
        if not os.path.exists(self.vector_store_path):
            return True
        if not os.path.isfile(sqlite_path):
            return True

        manifest = self._load_manifest()
        current_files = self._gather_source_files()
        current_sig = self._compute_signature(current_files)

        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ docs –≤–æ–æ–±—â–µ ‚Äî –Ω–µ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º (–∏–∑–±–µ–≥–∞–µ–º –ø—É—Å—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞)
        if len(current_files) == 0:
            # –ù–æ –µ—Å–ª–∏ —Ä–∞–Ω—å—à–µ –±—ã–ª–æ —á—Ç–æ-—Ç–æ (–≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ >0) ‚Äî –Ω–µ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º, –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ
            return False

        if manifest is None:
            print("[manifest] –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞")
            return True
        old_sig = manifest.get("signature")
        old_files = manifest.get("files", [])
        if old_sig != current_sig:
            print("[manifest] –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ -> –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞")
            return True

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–≤–µ–∂–µ—Å—Ç—å: –µ—Å–ª–∏ –ª—é–±–æ–π —Ñ–∞–π–ª –Ω–æ–≤–µ–µ sqlite
        try:
            index_mtime = os.path.getmtime(sqlite_path)
            for f in current_files:
                full = os.path.join(self.docs_path, f["rel"])
                if os.path.getmtime(full) > index_mtime:
                    print(f"[manifest] —Ñ–∞–π–ª {f['rel']} –Ω–æ–≤–µ–µ –∏–Ω–¥–µ–∫—Å–∞")
                    return True
        except Exception:
            pass

        return False

    # ================== –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –ú–ê–ù–ò–§–ï–°–¢–ê ==================
